<a name="readme-top"></a>
# IngenierÃ­a de Datos: Big Data

Este repositorio contiene los diferentes entornos de desarrollo que se utilizan para la asignatura **IngenierÃ­a de Datos: Big Data** del **MÃ¡ster en IngenierÃ­a del Software - Cloud, Datos y GestiÃ³n TI** de la Escuela TÃ©cnica Superior de IngenierÃ­a InformÃ¡tica de la Universidad de Sevilla.

## ğŸš€ CaracterÃ­sticas  

- ğŸ“Œ Entorno basado en Docker: FÃ¡cil despliegue y configuraciÃ³n de los servicios.

- ğŸ“ Soporte para Hadoop: EjecuciÃ³n de trabajos de MapReduce con ejemplos prÃ¡cticos.

- ğŸ” Hive y Trino: Consultas SQL sobre datos distribuidos en HDFS.

- ğŸ“¡ Kafka: Procesamiento en tiempo real con productores y consumidores.

- ğŸš€ Apache Airflow: OrquestaciÃ³n de flujos de datos con DAGs personalizados.

- ğŸ“Š Incluye datasets de prueba: Archivos de texto y CSV para experimentaciÃ³n.

- ğŸ”„ Modularidad: SeparaciÃ³n clara de los entornos en sesiones especÃ­ficas. 


## ğŸ“‚ Estructura del Repositorio  

```
ğŸ“‚ Proyecto
â”œâ”€â”€ ğŸ“‚ datasets/                     # Conjunto de datos utilizados en el proyecto
â”‚   â”œâ”€â”€ ğŸ“‚ E0/
â”‚   â”œâ”€â”€ ğŸ“‚ E1/
â”‚   â”œâ”€â”€ ğŸ“‚ E2/
â”‚   â”œâ”€â”€ ğŸ“„ quijote.txt
â”‚   â”œâ”€â”€ ğŸ“„ README.txt
â”‚
â”œâ”€â”€ ğŸ“‚ S1-hadoop/                    # ConfiguraciÃ³n y scripts para Hadoop (SesiÃ³n 1)
â”‚   â”œâ”€â”€ ğŸ“„ docker-compose.yml
â”‚   â”œâ”€â”€ ğŸ“„ dockerfile
â”‚   â”œâ”€â”€ ğŸ“‚ hadoop_config/
â”‚   â”œâ”€â”€ ğŸ“‚ scripts/
â”‚   â”œâ”€â”€ ğŸ“‚ src/MapReduce/
â”‚
â”œâ”€â”€ ğŸ“‚ S2-Hive y Trino/              # ConfiguraciÃ³n de Hive y Trino (SesiÃ³n 2)
â”‚   â”œâ”€â”€ ğŸ“„ docker-compose.yml
â”‚   â”œâ”€â”€ ğŸ“„ dockerfile
â”‚   â”œâ”€â”€ ğŸ“‚ hadoop_config/
â”‚   â”œâ”€â”€ ğŸ“‚ hive/
â”‚   â”œâ”€â”€ ğŸ“‚ trino-config/
â”‚   â”œâ”€â”€ ğŸ“‚ scripts/
â”‚   â”œâ”€â”€ ğŸ“‚ src/MapReduce/
â”‚
â”œâ”€â”€ ğŸ“‚ S3-Kafka/                     # ConfiguraciÃ³n de Kafka y productores/consumidores (SesiÃ³n 3)
â”‚   â”œâ”€â”€ ğŸ“„ docker-compose.yml
â”‚   â”œâ”€â”€ ğŸ“„ dockerfile
â”‚   â”œâ”€â”€ ğŸ“‚ hadoop_config/
â”‚   â”œâ”€â”€ ğŸ“‚ scripts/
â”‚   â”œâ”€â”€ ğŸ“‚ src/prod-cons/
â”‚
â”œâ”€â”€ ğŸ“‚ S4-Airflow/                   # ConfiguraciÃ³n de Apache Airflow (SesiÃ³n 4)
â”‚   â”œâ”€â”€ ğŸ“„ docker-compose.yml
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile.airflow
â”‚   â”œâ”€â”€ ğŸ“‚ hadoop_config/
â”‚   â”œâ”€â”€ ğŸ“‚ scripts/
â”‚   â”œâ”€â”€ ğŸ“‚ src/
â”‚
â”œâ”€â”€ ğŸ“„ .gitignore                    # Archivos y carpetas ignorados por Git
â”œâ”€â”€ ğŸ“„ README.md                      # DocumentaciÃ³n del repositorio
```

## ğŸ› ï¸ Requisitos  

- **Docker** y **Docker Compose** instalados en el sistema.  
- **RAM**: MÃ­nimo 8GB (recomendado 16GB+ para entornos completos).
- **Espacio en disco**: Al menos 20GB libres para contenedores y datos.

## âš¡ InstalaciÃ³n y Uso  

1ï¸âƒ£ Clona este repositorio:  
```sh
git clone https://github.com/josemarialuna/hdfs-docker-cluster.git
cd hdfs-docker-cluster
```

2ï¸âƒ£ Inicia el clÃºster de Hadoop con Docker Compose:  
```sh
docker-compose up -d
```

3ï¸âƒ£ Verifica que los contenedores estÃ¡n en ejecuciÃ³n:  
```sh
docker ps
```

4ï¸âƒ£ Accede al **Namenode** para interactuar con HDFS:  
```sh
docker exec -it namenode bash
```
<p align="right">(<a href="#readme-top">Volver arriba</a>)</p>

## ğŸ“Œ Comandos Ãštiles  

ğŸ”¹ Listar los archivos en HDFS:  
```sh
hdfs dfs -ls /
```

ğŸ”¹ Subir un archivo a HDFS:  
```sh
hdfs dfs -put archivo.txt /ruta/destino/
```

ğŸ”¹ Descargar un archivo de HDFS:  
```sh
hdfs dfs -get /ruta/origen/archivo.txt .
```

ğŸ”¹ Ver el estado del clÃºster:  
```sh
hdfs dfsadmin -report
```

ğŸ”¹ Salir manualmente del Safe Mode: 
```sh
hdfs dfsadmin -safemode leave
```

ğŸ”¹ Verificar replicaciÃ³n de bloques y balancear datos:
```sh
hdfs fsck /
hdfs balancer
```



<p align="right">(<a href="#readme-top">Volver arriba</a>)</p>


## ğŸ“ Notas  

- El sistema estÃ¡ configurado para un entorno de desarrollo, no para producciÃ³n.  
- Se pueden aÃ±adir mÃ¡s **Datanodes** editando el `docker-compose.yml`.  

##  FAQ  
**El namenode me da un error de unexpected end of file**
Verifica caracteres ocultos en el fichero. Ejecuta:
```sh
cat -A start-hdfs.sh
```
Si ves ^M al final de las lÃ­neas, el archivo tiene formato Windows y debes convertirlo.
```sh
sed -i 's/\r$//' start-hdfs.sh
```


## ğŸ“– Referencias  

- [DocumentaciÃ³n oficial de Hadoop](https://hadoop.apache.org/docs/stable/)  
- [Docker Hub - Hadoop Images](https://hub.docker.com/)  

<p align="right">(<a href="#readme-top">Volver arriba</a>)</p>


<!-- MARKDOWN LINKS & IMAGES -->
<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->
[contributors-shield]: https://img.shields.io/github/contributors/josemarialuna/hdfs-docker-cluster.svg?style=for-the-badge
[contributors-url]: https://github.com/josemarialuna/hdfs-docker-cluster/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/josemarialuna/hdfs-docker-cluster.svg?style=for-the-badge
[forks-url]: https://github.com/josemarialuna/hdfs-docker-cluster/network/members
[stars-shield]: https://img.shields.io/github/stars/josemarialuna/hdfs-docker-cluster.svg?style=for-the-badge
[stars-url]: https://github.com/josemarialuna/hdfs-docker-cluster/stargazers
[issues-shield]: https://img.shields.io/github/issues/josemarialuna/hdfs-docker-cluster.svg?style=for-the-badge
[issues-url]: https://github.com/josemarialuna/hdfs-docker-cluster/issues
[license-shield]: https://img.shields.io/github/license/josemarialuna/hdfs-docker-cluster.svg?style=for-the-badge
[license-url]: https://github.com/josemarialuna/hdfs-docker-cluster/blob/master/LICENSE.txt
[personal-shield]: https://img.shields.io/badge/Personal%20Site-555?style=for-the-badge
[personal-url]: https://josemarialuna.com