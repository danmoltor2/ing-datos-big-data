networks:
  airflow_network:
    driver: bridge
    
volumes:
  warehouse:
  mysql-db:

services:
  # Nodo Maestro de HDFS
  namenode:
    image: apache/hadoop:3.4.1
    container_name: namenode
    hostname: namenode
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    volumes:
      - ./hadoop_namenode:/opt/hadoop/data/nameNode
      - ./hadoop_config:/opt/hadoop/etc/hadoop
      - ./scripts/start-hdfs.sh:/start-hdfs.sh
    ports:
      - "9870:9870"
      - "9000:9000"
    command: ["/bin/bash", "/start-hdfs.sh"]
    networks:
      - airflow_network

  # Nodos de almacenamiento de HDFS
  datanode1:
    image: apache/hadoop:3.4.1
    container_name: datanode1
    hostname: datanode1
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    volumes:
      - ./hadoop_datanode1:/opt/hadoop/data/dataNode
      - ./hadoop_config:/opt/hadoop/etc/hadoop
      - ./scripts/init-datanode.sh:/init-datanode.sh
    depends_on:
      - namenode
    command: ["/bin/bash", "/init-datanode.sh"]
    networks:
      - airflow_network

  datanode2:
    image: apache/hadoop:3.4.1
    container_name: datanode2
    hostname: datanode2
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    volumes:
      - ./hadoop_datanode2:/opt/hadoop/data/dataNode
      - ./hadoop_config:/opt/hadoop/etc/hadoop
      - ./scripts/init-datanode.sh:/init-datanode.sh
    depends_on:
      - namenode
    command: ["/bin/bash", "/init-datanode.sh"]
    networks:
      - airflow_network

  # YARN Resource Manager
  resourcemanager:
    build: .
    container_name: resourcemanager
    hostname: resourcemanager
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    volumes:
      - ./hadoop_config:/opt/hadoop/etc/hadoop
      - ./scripts/start-yarn.sh:/start-yarn.sh
    ports:
      - "8088:8088"   # Interfaz web de YARN
      - "8030:8030"   # Comunicación con aplicaciones
      - "8031:8031"
      - "8032:8032"
      - "8033:8033"
    depends_on:
      - namenode
    command: ["/bin/bash", "/start-yarn.sh"]
    networks:
      - airflow_network

  # YARN Node Manager
  nodemanager:
    build: .
    container_name: nodemanager
    hostname: nodemanager
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    volumes:
      - ./hadoop_config:/opt/hadoop/etc/hadoop
      - ./scripts/start-nodemanager.sh:/start-nodemanager.sh
    depends_on:
      - resourcemanager
    command: ["/bin/bash", "/start-nodemanager.sh"]
    ports:
      - "8042:8042"   # Interfaz del NodeManager
    networks:
      - airflow_network

  # MySQL como metastore de Hive
  mysql:
    image: mysql:8.0
    restart: unless-stopped
    container_name: mysql
    hostname: mysql
    environment:
      MYSQL_ROOT_PASSWORD: rootpassword
      MYSQL_DATABASE: metastore_db
      MYSQL_USER: hive
      MYSQL_PASSWORD: password
      MYSQL_ROOT_HOST: "%"
      MYSQL_ALLOW_EMPTY_PASSWORD: "yes"
    ports:
      - "3306:3306"
    command: --default-authentication-plugin=mysql_native_password --require-secure-transport=OFF
    volumes:
      - mysql-db:/var/lib/mysql
    networks:
      - airflow_network


  # Metastore de Hive
  metastore:
    image: apache/hive:4.0.1
    depends_on:
      - mysql
    restart: unless-stopped
    container_name: metastore
    hostname: metastore
    environment:
      DB_DRIVER: mysql
      SERVICE_NAME: 'metastore'
      SERVICE_OPTS: '-Xmx1G -Djavax.jdo.option.ConnectionDriverName=com.mysql.cj.jdbc.Driver
                     -Djavax.jdo.option.ConnectionURL=jdbc:mysql://mysql:3306/metastore_db?useSSL=false
                     -Djavax.jdo.option.ConnectionUserName=hive
                     -Djavax.jdo.option.ConnectionPassword=password'
    ports:
      - '9083:9083'
    volumes:
      - warehouse:/opt/hive/data/warehouse
      - ./hive/conf/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./hive/conf/mysql-connector-j-8.0.33.jar:/opt/hive/lib/mysql.jar  # ⬅ Conector MySQL
    networks:
      - airflow_network

  # HiveServer2
  hiveserver2:
    image: apache/hive:4.0.1
    restart: unless-stopped
    container_name: hiveserver2
    hostname: hiveserver2
    depends_on:
      - metastore
    environment:
      SERVICE_NAME: hiveserver2
      HIVE_SERVER2_THRIFT_PORT: 10000
      HIVE_METASTORE_URIS: thrift://metastore:9083
      DB_TYPE: mysql
      DB_CONNECTION_URL: jdbc:mysql://mysql:3306/metastore_db?useSSL=false&allowPublicKeyRetrieval=true&serverTimezone=UTC
      DB_USER: hive
      DB_PASSWORD: password
    ports:
      - "10000:10000"
      - "10002:10002"
    volumes:
      - warehouse:/opt/hive/data/warehouse
      - ./hive/conf/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./hive/conf/mysql-connector-j-8.0.33.jar:/opt/hive/lib/mysql.jar
    command: ["/bin/bash", "-c", "chmod -R 777 /user/hive/data/warehouse && /opt/hive/bin/hive --service hiveserver2"]
    networks:
      - airflow_network


  # Broker de Kafka
  kafka:
    image: confluentinc/cp-kafka:7.9.0
    container_name: kafka
    hostname: kafka
    depends_on:
      - zookeeper
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:9093
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT_HOST://localhost:9093,PLAINTEXT://kafka:9092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
    ports:
      - "9092:9092"
      - "9093:9093"
    networks:
      - airflow_network

  # Zookeeper para la coordinación de Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.9.0
    container_name: zookeeper
    hostname: zookeeper
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
    networks:
      - airflow_network

  # Kafka Connect para integración con otros sistemas
  kafka-connect:
    build:
      context: .
      dockerfile: Dockerfile.kafka-connect
    container_name: kafka-connect
    hostname: kafka-connect
    depends_on:
      - kafka
    environment:
    - CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR=1
    - CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR=1
    - CONNECT_STATUS_STORAGE_REPLICATION_FACTOR=1
    - CONNECT_BOOTSTRAP_SERVERS=kafka:9092
    - CONNECT_REST_PORT=8083
    - CONNECT_GROUP_ID=connect-cluster
    - CONNECT_CONFIG_STORAGE_TOPIC=connect-configs
    - CONNECT_OFFSET_STORAGE_TOPIC=connect-offsets
    - CONNECT_STATUS_STORAGE_TOPIC=connect-status
    - CONNECT_KEY_CONVERTER=org.apache.kafka.connect.storage.StringConverter
    - CONNECT_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
    - CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE=false
    - CONNECT_REST_ADVERTISED_HOST_NAME=kafka-connect
    ports:
      - "8083:8083"
    networks:
      - airflow_network


  # Interfaz web para administración de Kafka
  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    hostname: kafka-ui
    depends_on:
      - kafka
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAP_SERVERS=kafka:9092
    ports:
      - "8080:8080"
    networks:
      - airflow_network
  
  # apache/airflow:2.9.3-python3.9
  airflow:
    build:
      context: .
      dockerfile: Dockerfile.airflow  # Usaremos un Dockerfile para instalar dependencias
    container_name: airflow
    hostname: airflow
    user: root
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor  # SQLite solo soporta SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__FERNET_KEY='d6nNXtWJQL85dCkOTB8FRQ5TPAi6XW7Xbw3l84QJ8q8='
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./airflow:/opt/airflow
    entrypoint: >
      /bin/bash -c "
      airflow db migrate &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true &&
      airflow connections add hdfs_default --conn-type webhdfs --conn-host namenode --conn-port 50070 --conn-extra '{\"proxy_user\": \"hdfs\"}' || true &&
      airflow connections add hive_default --conn-type hive --conn-host hiveserver2 --conn-port 10000 --conn-login hive --conn-password password || true &&
      airflow connections add kafka_default --conn-type kafka --conn-host kafka --conn-port 9092 --conn-extra '{\"bootstrap.servers\": \"kafka:9092\", \"group.id\": \"airflow-consumer\"}' || true &&
      airflow scheduler & airflow webserver --port 8080"
    ports:
      - "8081:8080"  # UI de Airflow
    networks:
      - airflow_network
        



